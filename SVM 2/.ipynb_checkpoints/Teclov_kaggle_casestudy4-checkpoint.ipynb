{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digits - Classification Using SVM\n",
    "\n",
    "In this notebook, we'll explore the popular MNIST dataset and build an SVM model to classify handwritten digits. <a href='http://yann.lecun.com/exdb/mnist/'>Here is a detailed description of the dataset.</a>\n",
    "\n",
    "We'll divide the analysis into the following parts:\n",
    "- Data understanding and cleaning\n",
    "- Data preparation for model building\n",
    "- Building an SVM model - hyperparameter tuning, model evaluation etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding and Cleaning\n",
    " \n",
    " Let's understand the dataset and see if it needs some cleaning etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import opencv2 as cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "digits = pd.read_csv(\"train.csv\")\n",
    "digits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# head\n",
    "digits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four = digits.iloc[3, 1:]\n",
    "four.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "four = four.values.reshape(28, 28)\n",
    "plt.imshow(four, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note: Indexing Recall ####\n",
    "`list =    [0, 4, 2, 10, 22, 101, 10]` <br>\n",
    "`indices = [0, 1, 2, 3, ...,        ]` <br>\n",
    "`reverse = [-n           -3  -2   -1]` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the array\n",
    "print(four[5:-5, 5:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise the counts of 'label' to see how many labels of each digit are present\n",
    "digits.label.astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise count in terms of percentage \n",
    "100*(round(digits.label.astype('category').value_counts()/len(digits.index), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, each digit/label has an approximately 9%-11% fraction in the dataset and the **dataset is balanced**. This is an important factor in considering the choices of models to be used, especially SVM, since **SVMs rarely perform well on imbalanced data** (think about why that might be the case).\n",
    "\n",
    "Let's quickly look at missing values, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values - there are none\n",
    "digits.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's look at the average values of each column, since we'll need to do some rescaling in case the ranges vary too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# average values/distributions of features\n",
    "description = digits.describe()\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the max value of the mean and maximum values of some features (pixels) is 139, 255 etc., whereas most features lie in much lower ranges  (look at description of pixel 0, pixel 1 etc. above).\n",
    "\n",
    "Thus, it seems like a good idea to rescale the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Model Building\n",
    "\n",
    "Let's now prepare the dataset for building the model. We'll only use a fraction of the data else training will take a long time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and test sets\n",
    "# Splitting the data into train and test\n",
    "X = digits.iloc[:, 1:]\n",
    "Y = digits.iloc[:, 0]\n",
    "\n",
    "# Rescaling the features\n",
    "from sklearn.preprocessing import scale\n",
    "X = scale(X)\n",
    "\n",
    "# train test split with train_size=10% and test size=90%\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.10, random_state=101)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete test set from memory, to avoid a memory error\n",
    "# we'll anyway use CV to evaluate the model, and can use the separate test.csv file as well\n",
    "# to evaluate the model finally\n",
    "\n",
    "# del x_test\n",
    "# del y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "Let's now build the model and tune the hyperparameters. Let's start with a **linear model** first.\n",
    "\n",
    "### Linear SVM\n",
    "\n",
    "Let's first try building a linear SVM model (i.e. a linear kernel). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "# an initial SVM model with linear kernel   \n",
    "svm_linear = svm.SVC(kernel='linear')\n",
    "\n",
    "# fit\n",
    "svm_linear.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = svm_linear.predict(x_test)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation: accuracy\n",
    "# C(i, j) represents the number of points known to be in class i \n",
    "# but predicted to be in class j\n",
    "confusion = metrics.confusion_matrix(y_true = y_test, y_pred = predictions)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# measure accuracy\n",
    "metrics.accuracy_score(y_true=y_test, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class-wise accuracy\n",
    "class_wise = metrics.classification_report(y_true=y_test, y_pred=predictions)\n",
    "print(class_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gc.collect() (garbage collect) to free up memory\n",
    "# else, since the dataset is large and SVM is computationally heavy,\n",
    "# it'll throw a memory error while training\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear SVM\n",
    "\n",
    "Let's now try a non-linear model with the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbf kernel with other hyperparameters kept to default \n",
    "svm_rbf = svm.SVC(kernel='rbf')\n",
    "svm_rbf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = svm_rbf.predict(x_test)\n",
    "\n",
    "# accuracy \n",
    "print(metrics.accuracy_score(y_true=y_test, y_pred=predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy achieved with a non-linear kernel is slightly higher than a linear one. Let's now do a grid search CV to tune the hyperparameters C and gamma.\n",
    "\n",
    "### Grid Search Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct (grid search) cross-validation to find the optimal values \n",
    "# of cost C and the choice of kernel\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'C':[1, 10, 100], \n",
    "             'gamma': [1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "# instantiate a model \n",
    "svc_grid_search = svm.SVC(kernel=\"rbf\")\n",
    "\n",
    "# create a classifier to perform grid search\n",
    "clf = GridSearchCV(svc_grid_search, param_grid=parameters, scoring='accuracy')\n",
    "\n",
    "# fit\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "cv_results = pd.DataFrame(clf.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting C to numeric type for plotting on x-axis\n",
    "cv_results['param_C'] = cv_results['param_C'].astype('int')\n",
    "\n",
    "# # plotting\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "# subplot 1/3\n",
    "plt.subplot(131)\n",
    "gamma_01 = cv_results[cv_results['param_gamma']==0.01]\n",
    "\n",
    "plt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\n",
    "plt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.01\")\n",
    "plt.ylim([0.60, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='lower right')\n",
    "plt.xscale('log')\n",
    "\n",
    "# subplot 2/3\n",
    "plt.subplot(132)\n",
    "gamma_001 = cv_results[cv_results['param_gamma']==0.001]\n",
    "\n",
    "plt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\n",
    "plt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.001\")\n",
    "plt.ylim([0.60, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='lower right')\n",
    "plt.xscale('log')\n",
    "\n",
    "\n",
    "# subplot 3/3\n",
    "plt.subplot(133)\n",
    "gamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n",
    "\n",
    "plt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\n",
    "plt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.0001\")\n",
    "plt.ylim([0.60, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='lower right')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can observe that (from higher to lower gamma / left to right):\n",
    "- At very high gamma (0.01), the model is achieving 100% accuracy on the training data, though the test score is quite low (<75%). Thus, the model is overfitting.\n",
    "\n",
    "- At gamma=0.001, the training and test scores are comparable at around C=1, though the model starts to overfit at higher values of C\n",
    "\n",
    "- At gamma=0.0001, the model does not overfit till C=10 but starts showing signs at C=100. Also, the training and test scores are slightly lower than at gamma=0.001.\n",
    "\n",
    "Thus, it seems that the best combination is gamma=0.001 and C=1 (the plot in the middle), which gives the highest test accuracy (~92%) while avoiding overfitting.\n",
    "\n",
    "Let's now build the final model and see the performance on test data.\n",
    "\n",
    "### Final Model\n",
    "\n",
    "Let's now build the final model with chosen hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal hyperparameters\n",
    "best_C = 1\n",
    "best_gamma = 0.001\n",
    "\n",
    "# model\n",
    "svm_final = svm.SVC(kernel='rbf', C=best_C, gamma=best_gamma)\n",
    "\n",
    "# fit\n",
    "svm_final.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = svm_final.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation: CM \n",
    "confusion = metrics.confusion_matrix(y_true = y_test, y_pred = predictions)\n",
    "\n",
    "# measure accuracy\n",
    "test_accuracy = metrics.accuracy_score(y_true=y_test, y_pred=predictions)\n",
    "\n",
    "print(test_accuracy, \"\\n\")\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The final accuracy on test data is approx. 92%. Note that this can be significantly increased by using the entire training data of 42,000 images (we have used just 10% of that!). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_manual)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
